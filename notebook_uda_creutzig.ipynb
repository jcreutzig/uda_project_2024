{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user specific setup \n",
    "# this should work fine as default but you might want to store data \n",
    "# other places so feel free to change the paths here\n",
    "\n",
    "zip_folder = 'data_download_zip/' \n",
    "dl_data_folder = 'data/'\n",
    "\n",
    "base_url = 'https://zenodo.org/record/8204334/files/'\n",
    "\n",
    "ds20_url = base_url + 'ds20.zip'\n",
    "ds100_url = base_url + 'ds100.zip'\n",
    "\n",
    "ds20_zipfile = zip_folder + 'ds20.zip'\n",
    "ds100_zipfile = zip_folder  + 'ds100.zip'\n",
    "\n",
    "ds20_data_folder = dl_data_folder + 'ds20/'\n",
    "ds100_data_folder = dl_data_folder + 'ds100/'\n",
    "\n",
    "ds20_model_folder = 'models/ds20/'\n",
    "ds100_model_folder = 'models/ds100/'\n",
    "\n",
    "EDGE_WGT = 9. \n",
    "\n",
    "our_rando_seed = 417417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### \n",
    "#  Actiuve user preferences \n",
    "# \n",
    "# Flags to activate/deactivate download, training, plotting etc \n",
    "#\n",
    "\n",
    "download_data = False # True always downloads/extracts data, False only when data is missing \n",
    "\n",
    "# we made an iterative process for training models\n",
    "# This can take quite some time, so you can instead also load pre-trained models.  \n",
    "make_iterative_models = False  \n",
    "\n",
    "# This runs simply through a grid of model hyperparameters and trains them all.\n",
    "# Also quite time intensive, and I think outdated now because the iterative process \n",
    "# works reasonably well.  \n",
    "make_grid_models = False \n",
    "\n",
    "# Which model to select for evaluation/plotting \n",
    "# You can get the best \"iterative\" or \"grid\" or a tuple (nl, nc, nh) for a specific model \n",
    "\n",
    "model_select = \"iterative\" \n",
    "# model_select = [(7,40,7), (8,40,9)] \n",
    "\n",
    "plot_results = True   # Plots the results of the training.\n",
    "\n",
    "# Training setup.  \n",
    "NL = [4, 5, 6, 7] # number of layers to be trained \n",
    "NC = [20, 30, 40] # number of channels \n",
    "NH = [4, 5, 6, 7] # number of hops \n",
    "\n",
    "hp_idx = [\n",
    "    (nl, nc, nh) \n",
    "    for nl in NL \n",
    "    for nc in NC  \n",
    "    for nh in NH \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict,  List, Callable\n",
    "import os \n",
    "import json \n",
    "from requests import get \n",
    "import zipfile\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import networkx as nx \n",
    "\n",
    "import torch \n",
    "from torch import tensor\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from torch_geometric.nn import TAGConv\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data \n",
    "# This will take maybe 30 seconds initially.  \n",
    "# Seecond time you should not notice the lag.  \n",
    "\n",
    "if download_data:\n",
    "\n",
    "    def mkdir_if_not_exist(folder):\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "    def make_path_if_not_exist(file):\n",
    "        folder = os.path.dirname(file)\n",
    "        mkdir_if_not_exist(folder)\n",
    "\n",
    "    def load_from_source(url, file, overwrite=False):\n",
    "        make_path_if_not_exist(file)\n",
    "        if not os.path.exists(file) or overwrite:\n",
    "            with open(file, 'wb') as f:\n",
    "                response = get(url)\n",
    "                f.write(response.content)\n",
    "                print(f\"Downloaded {url} to {file}\")\n",
    "                f.close()\n",
    "\n",
    "        else:\n",
    "            print(f\"File {file} already exists. Set overwrite=True to overwrite\")\n",
    "\n",
    "    load_from_source(ds20_url, ds20_zipfile)\n",
    "    load_from_source(ds100_url, ds100_zipfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now extract from zip files and load the data \n",
    "if download_data: \n",
    "    def extract_zip(zipfile_name, folder):\n",
    "        zf = zipfile.ZipFile(zipfile_name)\n",
    "        zf.extractall(folder)\n",
    "        zf.close()\n",
    "\n",
    "    extract_zip(ds20_zipfile, dl_data_folder)\n",
    "    extract_zip(ds100_zipfile, dl_data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nbjce1\\Anaconda3\\envs\\uda_project_autumn_2024\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# read in hd5 data \n",
    "def load_data(ds: str, type:str, data_folder:str, batch_size=500) -> DataLoader:  \n",
    "\n",
    "    folder_name = f\"{data_folder}/{ds}/{type}/\"\n",
    "\n",
    "    file_name_input = f\"{folder_name}/input_data.h5\"\n",
    "    file_name_snbs = f\"{folder_name}/snbs.h5\"\n",
    "\n",
    "    snbs_data = h5py.File(file_name_snbs, 'r')\n",
    "    input_data = h5py.File(file_name_input, 'r')['grids']\n",
    "\n",
    "    grid_data = [\n",
    "        Data(\n",
    "            x = tensor(np.array(id_v['node_features']).reshape(-1, 1), dtype=torch.float),\n",
    "            edge_index = tensor(np.array(id_v['edge_index']).T - 1, dtype=torch.long),\n",
    "            edge_attr = tensor(np.full(shape=id_v['edge_index'].shape[0], fill_value=EDGE_WGT), dtype=torch.float),\n",
    "            y = tensor(np.array(snbs_v), dtype=torch.float)\n",
    "        )\n",
    "        for id_v, snbs_v in zip(input_data.values(), snbs_data.values())\n",
    "    ]\n",
    "\n",
    "    return DataLoader(grid_data, batch_size=batch_size)\n",
    "\n",
    "def load_dataset(ds:str, data_folder: str) -> Dict[str, DataLoader]: \n",
    "    res= dict() \n",
    "    res['train_ds'] = load_data(ds=ds, type='train', data_folder=data_folder)\n",
    "    res['test_ds'] = load_data(ds=ds, type='test', data_folder=data_folder)\n",
    "    res['valid_ds'] = load_data(ds=ds, type='valid', data_folder=data_folder)\n",
    "    return res \n",
    "\n",
    "data_ds20 = load_dataset('ds20', dl_data_folder)\n",
    "data_ds100 = load_dataset('ds100', dl_data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[20, 1], edge_index=[2, 54], edge_attr=[54], y=[20])\n",
      "tensor([[-1.,  1.,  1., -1., -1.]])\n",
      "tensor([[0, 1, 0, 3, 0],\n",
      "        [1, 0, 3, 0, 5]])\n",
      "tensor([9., 9., 9., 9., 9.])\n",
      "tensor([1.0000, 1.0000, 0.8447, 1.0000, 0.9946])\n"
     ]
    }
   ],
   "source": [
    "# This is how the training data looks like\n",
    "\n",
    "print(data_ds20['train_ds'].dataset[0])\n",
    "print(data_ds100['train_ds'].dataset[0].x[:5].T) \n",
    "print(data_ds100['train_ds'].dataset[0].edge_index[:2,:5])\n",
    "print(data_ds100['train_ds'].dataset[0].edge_attr[:5])\n",
    "print(data_ds100['train_ds'].dataset[0].y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAGConvModule(torch.nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, K_hops, activation, batch_norm) -> None:\n",
    "        super().__init__()\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "        self.K_hops = K_hops\n",
    "        self.activation = activation\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.conv = TAGConv(\n",
    "            self.channels_in, \n",
    "            self.channels_out, \n",
    "            K=self.K_hops\n",
    "        )\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm_layer = torch.nn.BatchNorm1d(self.channels_out)\n",
    "\n",
    "    def forward(self, data, x):\n",
    "        # Add safety checks\n",
    "        num_nodes = x.size(0)\n",
    "        if data.edge_index.max() >= num_nodes:\n",
    "            raise ValueError(f\"Edge index contains invalid node indices. Max index: {data.edge_index.max()}, num nodes: {num_nodes}\")\n",
    "        \n",
    "        if data.edge_index.min() < 0:\n",
    "            raise ValueError(f\"Edge index contains negative indices: {data.edge_index.min()}\")\n",
    "            \n",
    "        # Apply convolution\n",
    "        x = self.conv(x, data.edge_index, data.edge_attr)\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm_layer(x)\n",
    "        \n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "class TAGNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        no_layers: int,\n",
    "        channels: List[int],\n",
    "        activation: List[Callable],\n",
    "        K_hops: List[int],\n",
    "        batch_norm: List[bool],\n",
    "        final_linear_layer: bool,\n",
    "        final_sigmoid_layer: bool = True, \n",
    "    ) -> None:\n",
    "        super(TAGNet, self).__init__()\n",
    "\n",
    "        self.no_layers = no_layers\n",
    "        self.channels = channels\n",
    "        self.activation = activation\n",
    "        self.K_hops = K_hops\n",
    "        self.batch_norm = batch_norm\n",
    "        self.final_linear_layer = final_linear_layer\n",
    "        self.final_sigmoid_layer = final_sigmoid_layer\n",
    "\n",
    "\n",
    "        self.convlist = torch.nn.ModuleList([\n",
    "            TAGConvModule(\n",
    "                channels_in=self.channels[i],\n",
    "                channels_out=self.channels[i+1],\n",
    "                activation=self.activation[i],\n",
    "                K_hops=self.K_hops[i],\n",
    "                batch_norm=self.batch_norm[i]\n",
    "            )\n",
    "            for i in range(self.no_layers)\n",
    "        ])\n",
    "\n",
    "        if self.final_linear_layer:\n",
    "            self.endLinear = torch.nn.Linear(self.channels[-1], 1)\n",
    "        if self.final_sigmoid_layer: \n",
    "            self.endSigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        \n",
    "        for conv in self.convlist:\n",
    "            x = conv(data, x)\n",
    "            \n",
    "        if self.final_linear_layer:\n",
    "            x = self.endLinear(x)\n",
    "        if self.final_sigmoid_layer: \n",
    "            x = self.endSigmoid(x)\n",
    "        \n",
    "        return x.squeeze(-1)  # Match target shape\n",
    "\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "class TAGModule(torch.nn.Module): \n",
    "    def __init__(\n",
    "        self, \n",
    "        channels, \n",
    "        activation,\n",
    "        K_hops,\n",
    "        batch_norm,\n",
    "        final_linear_layer, \n",
    "        final_sigmoid_layer\n",
    "    ) -> None:\n",
    "        \n",
    "        torch.manual_seed(our_rando_seed)\n",
    "        torch.cuda.manual_seed(our_rando_seed)\n",
    "        np.random.seed(our_rando_seed)        \n",
    "\n",
    "        super(TAGModule, self).__init__()\n",
    "        self.model = TAGNet(\n",
    "            no_layers=len(channels) - 1,\n",
    "            channels=channels,\n",
    "            activation=activation,\n",
    "            K_hops=K_hops,\n",
    "            batch_norm=batch_norm,\n",
    "            final_linear_layer=final_linear_layer,\n",
    "            final_sigmoid_layer=final_sigmoid_layer\n",
    "        )\n",
    "        self.device = torch.device('cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.criterion = MSELoss(reduction='mean')\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(), lr=3, momentum=0.9\n",
    "        ) \n",
    "\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=10, gamma=0.1\n",
    "        )\n",
    "\n",
    "        self.r2score = R2Score().to(self.device)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.model(x)\n",
    "\n",
    "    def r2score(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Calculate R² score from predictions and labels\n",
    "        Args:\n",
    "            predictions: torch.Tensor of predicted values\n",
    "            labels: torch.Tensor of true values\n",
    "        Returns:\n",
    "            R² score as a float\n",
    "        \"\"\"\n",
    "        # Mean of true values\n",
    "        y_mean = torch.mean(labels)\n",
    "        \n",
    "        # Total sum of squares\n",
    "        ss_tot = torch.sum((labels - y_mean) ** 2)\n",
    "        \n",
    "        # Residual sum of squares\n",
    "        ss_res = torch.sum((labels - predictions) ** 2)\n",
    "        \n",
    "        # R² calculation\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        return r2.item()\n",
    "\n",
    "    def train_epoch(self, data_loader, threshold=0.1): \n",
    "        self.model.train() \n",
    "        all_labels = torch.Tensor(0).to(self.device)\n",
    "        all_preds = torch.Tensor(0).to(self.device)\n",
    "        correct = 0 \n",
    "        for _, (batch) in enumerate(data_loader):\n",
    "            batch.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = torch.squeeze(self.model.forward(batch))\n",
    "            loss = self.criterion(outputs, batch.y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            correct += torch.sum(( torch.abs(outputs-batch.y) < threshold ))\n",
    "            all_labels = torch.cat([all_labels, batch.y])\n",
    "            all_preds = torch.cat([all_preds, outputs])\n",
    "\n",
    "        accuracy = correct / all_labels.shape[0]\n",
    "        r2_score = self.r2score(all_preds, all_labels)\n",
    "\n",
    "        return accuracy, r2_score\n",
    "\n",
    "    def eval_model(self, data_loader, threshold=0.1): \n",
    "        self.model.eval() \n",
    "        correct = 0 \n",
    "        all_labels = torch.Tensor(0).to(self.device)\n",
    "        all_preds = torch.Tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for _, (batch) in enumerate(data_loader):\n",
    "                batch.to(self.device)\n",
    "                outputs = torch.squeeze(self.model(batch))\n",
    "                correct += torch.sum(( torch.abs(outputs-batch.y) < threshold ))\n",
    "                all_labels = torch.cat([all_labels, batch.y])\n",
    "                all_preds = torch.cat([all_preds, outputs])\n",
    "\n",
    "        accuracy = correct / all_labels.shape[0]\n",
    "        r2_score = self.r2score(all_preds, all_labels)\n",
    "\n",
    "        return accuracy, r2_score\n",
    "    \n",
    "    def pred_v_actual(self, data_loader):\n",
    "        \"\"\"\n",
    "        Extract predictions and ground truth from a data loader\n",
    "        Args:\n",
    "            data_loader: PyTorch Geometric DataLoader\n",
    "        Returns:\n",
    "            tuple (predictions, ground_truth) as numpy arrays\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        all_labels = torch.Tensor(0).to(self.device)\n",
    "        all_preds = torch.Tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _, (batch) in enumerate(data_loader):\n",
    "                batch.to(self.device)\n",
    "                outputs = torch.squeeze(self.model(batch))\n",
    "                all_labels = torch.cat([all_labels, batch.y])\n",
    "                all_preds = torch.cat([all_preds, outputs])\n",
    "\n",
    "        # Convert to numpy arrays for easier handling\n",
    "        return all_preds.cpu().numpy(), all_labels.cpu().numpy()       \n",
    "\n",
    "# Update your model initialization\n",
    "tag_net = TAGNet(\n",
    "    no_layers=3,\n",
    "    channels=[1, 32, 32, 1],  # Make sure dimensions match\n",
    "    activation=[torch.nn.ReLU()] * 3,\n",
    "    K_hops=[2] * 3,\n",
    "    batch_norm=[True] * 3, \n",
    "    final_linear_layer=True\n",
    ")\n",
    "\n",
    "tag_module = TAGModule(\n",
    "    channels=[1, 30, 30, 1],\n",
    "    activation=[torch.nn.ReLU()] * 3,\n",
    "    K_hops=[3] * 3,\n",
    "    batch_norm=[True] * 3,\n",
    "    final_linear_layer=False,\n",
    "    final_sigmoid_layer=True\n",
    ")\n",
    "def triangle_channels(nl, nc):  \n",
    "    return nc // 2**(nl-3 - np.arange(nl-2))\n",
    "\n",
    "\n",
    "def make_tag_module(num_layers, num_channels, num_hops): \n",
    "    return TAGModule(\n",
    "        channels=[1] + triangle_channels(num_layers, num_channels)  + [1],\n",
    "        activation=[torch.nn.ReLU()] * num_layers,\n",
    "        K_hops=[num_hops] * num_layers,\n",
    "        batch_norm=[True] * num_layers,\n",
    "        final_linear_layer=False,\n",
    "        final_sigmoid_layer=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use now TAGModule to train model \n",
    "\n",
    "def train_model(tag_module, data_loader, epochs, patience_limit=50, min_perc_increase=0, print_at=None):\n",
    "    best_so_far = -np.inf\n",
    "    patience_used=0\n",
    "    all_train_acc = [] \n",
    "    all_train_r2 = [] \n",
    "    all_test_acc = [] \n",
    "    all_test_r2 = [] \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_acc, train_r2 = tag_module.train_epoch(data_loader['train_ds'], threshold=0.05)\n",
    "        valid_acc, valid_r2 = tag_module.eval_model(data_loader['valid_ds'], threshold=0.05)\n",
    "        if valid_r2 > best_so_far * (1 + min_perc_increase/100):\n",
    "            best_so_far = valid_r2\n",
    "            patience_used = 0\n",
    "        else:\n",
    "            patience_used += 1\n",
    "            if patience_used > patience_limit :\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        if print_at: \n",
    "            if epoch % print_at == 0:\n",
    "                print(f\"Epoch {epoch}: Train accuracy: {train_acc:.2f}, Train R2: {train_r2:.2f}, Valid accuracy: {valid_acc:.2f}, Valid R2: {valid_r2:.2f}\")\n",
    "\n",
    "        all_train_acc.append(float(train_acc))\n",
    "        all_train_r2.append(float(train_r2.item()))\n",
    "        all_test_acc.append(float(valid_acc))\n",
    "        all_test_r2.append(float(valid_r2.item()))\n",
    "\n",
    "    hist = {\n",
    "        'train_acc': np.array(all_train_acc), \n",
    "        'train_r2': np.array(all_train_r2), \n",
    "        'valid_acc': np.array(all_test_acc), \n",
    "        'valid_r2': np.array(all_test_r2), \n",
    "    }\n",
    "    return hist \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "        index_set, \n",
    "        which='ds20', \n",
    "        overwrite=False, \n",
    "        epochs=400, \n",
    "        patience_limit=400, \n",
    "        min_perc_increase = 0 \n",
    "        ): \n",
    "    hist_dict = dict()\n",
    "    model_dict = dict()\n",
    "    \n",
    "    training_folder = f'trained_models_{which}/'\n",
    "    hist_folder = f'training_hist_{which}/'\n",
    "\n",
    "    # make folders if not exist \n",
    "    mkdir_if_not_exist(training_folder)\n",
    "    mkdir_if_not_exist(hist_folder)\n",
    "\n",
    "    for idx in index_set: \n",
    "        training_file_name = training_folder + f'model_{str(idx)}.pth'\n",
    "        if os.path.exists(training_file_name) and not overwrite:\n",
    "            print(f\"Model {str(idx)} already trained. Skipping\")\n",
    "            continue\n",
    "\n",
    "        # find data set \n",
    "        if which == 'ds20': \n",
    "            data = data_ds20 \n",
    "        elif which == 'ds100': \n",
    "            data = data_ds100\n",
    "        else:\n",
    "            raise ValueError('Unknown parameter which ' + which)\n",
    "\n",
    "        print(f'Training {str(idx)}..')\n",
    "        model_dict[idx] = make_tag_module(\n",
    "            num_layers=idx[0], \n",
    "            num_channels=idx[1], \n",
    "            num_hops = idx[2]\n",
    "            )\n",
    "        hist_dict[idx] = train_model(\n",
    "            model_dict[idx], \n",
    "            data, \n",
    "            epochs = epochs, \n",
    "            patience_limit=patience_limit, \n",
    "            print_at=50, \n",
    "            min_perc_increase = min_perc_increase\n",
    "        )\n",
    "\n",
    "        torch.save(model_dict[idx], training_folder + f'model_{str(idx)}.pth') \n",
    "        # transform hist dict into json and save too \n",
    "        tmp_dict = {\n",
    "            k: list(v) \n",
    "            for k, v in hist_dict[idx].items() \n",
    "        }\n",
    "        import json \n",
    "        json.dump(\n",
    "            tmp_dict, \n",
    "            open(f'{hist_folder}/{str(idx)}.pth', 'w') \n",
    "            )\n",
    "\n",
    "    print(\"done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_hp_tuning(\n",
    "\n",
    "    nl_init = 2, \n",
    "    nc_init = 10, \n",
    "    nh_init = 1, \n",
    "\n",
    "    inc_nl = 1, \n",
    "    inc_nc = 5,\n",
    "    inc_nh = 1,\n",
    "\n",
    "    max_depth = 20, \n",
    "    patience = 15,\n",
    "    min_perc_increase=5, \n",
    "\n",
    "    which = 'ds20'\n",
    "): \n",
    "    nl, nc, nh = (nl_init, nc_init, nh_init)\n",
    "    chain_of_champions = [(nl, nc, nh)]\n",
    "\n",
    "    hit_limit = False \n",
    "\n",
    "    import json \n",
    "    def get_r2(idx, which='ds20'): \n",
    "        hist_folder = f'training_hist_{which}'\n",
    "        perf = json.load(open(f'{hist_folder}/{str(idx)}.pth'))\n",
    "        return perf['valid_r2'][-1]\n",
    "\n",
    "    def train_and_get_r2(idx, which='ds20'): \n",
    "        train_batch([idx], which=which, epochs=400, patience_limit=patience, min_perc_increase = min_perc_increase) \n",
    "        return(get_r2(idx, which))\n",
    "\n",
    "    def argmax_dict(my_dict): \n",
    "        \"\"\"returns key with largest value.  Only works with comparable values..\"\"\"\n",
    "        list_idx_max = np.argmax(list(my_dict.values()))\n",
    "        return list(my_dict.keys())[list_idx_max]\n",
    "\n",
    "    counter= 0 \n",
    "    max_count = max_depth \n",
    "\n",
    "    while not hit_limit and counter < max_count: \n",
    "        curr_perf = {\n",
    "            idx: train_and_get_r2(idx, which) \n",
    "            for idx in [\n",
    "                (nl, nc, nh), \n",
    "                (nl + inc_nl, nc, nh), \n",
    "                (nl, nc + inc_nc, nh), \n",
    "                (nl, nc, nh + inc_nh), \n",
    "            ]\n",
    "        }\n",
    "        idx_with_max_perf = argmax_dict(curr_perf) \n",
    "        if idx_with_max_perf == (nl, nc, nh): \n",
    "            print(f\"No further improvement registered.  Final performance {curr_perf[idx_with_max_perf]}.\")\n",
    "            hit_limit=True \n",
    "        else: \n",
    "            nl, nc, nh = idx_with_max_perf \n",
    "            chain_of_champions.append(idx_with_max_perf)\n",
    "            print(f\"=====  New champion: {idx_with_max_perf} =====\")\n",
    "            counter += 1\n",
    "\n",
    "    return chain_of_champions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hist_data(idx, which: str='ds20'):\n",
    "    hist_folder = f'training_hist_{which}'\n",
    "    return json.load(open(f'{hist_folder}/{str(idx)}.pth'))\n",
    "\n",
    "def load_tag_model(idx, which: str='ds20'): \n",
    "    training_folder = f'trained_models_{which}'\n",
    "    return torch.load(f'{training_folder}/model_{str(idx)}.pth')\n",
    "\n",
    "def all_indices(which: str='ds20'): \n",
    "    training_folder = f'trained_models_{which}'\n",
    "    return [tuple(json.loads(\n",
    "        '[' + f.replace('model_','').split('.')[0][1:-1] + ']'\n",
    "        )) for f in os.listdir(training_folder)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'champions_20.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(chain_of_champions_100, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchampions_100.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     chain_of_champions_20 \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchampions_20.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      9\u001b[0m     chain_of_champions_100 \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchampions_100.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     12\u001b[0m tm_best_it_20 \u001b[38;5;241m=\u001b[39m make_tag_module(\u001b[38;5;241m*\u001b[39mchain_of_champions_20[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'champions_20.json'"
     ]
    }
   ],
   "source": [
    "if make_iterative_models: \n",
    "    chain_of_champions_20 = iterative_hp_tuning(which='ds20', nl=3, nc = 10, nh=2, min_perc_increase=0.25, patience = 15)\n",
    "    chain_of_champions_100 = iterative_hp_tuning(which='ds100', nl=3, nc = 10, nh=2, min_perc_increase=0.25, patience = 15)\n",
    "    # save the champions\n",
    "    json.dump(chain_of_champions_20, open('champions_20.json', 'w'))\n",
    "    json.dump(chain_of_champions_100, open('champions_100.json', 'w'))\n",
    "else:\n",
    "    chain_of_champions_20 = json.load(open('champions_20.json'))\n",
    "    chain_of_champions_100 = json.load(open('champions_100.json'))\n",
    "\n",
    "\n",
    "tm_best_it_20 = load_tag_model(*chain_of_champions_20[-1], which='ds20')\n",
    "tm_best_it_100 = load_tag_model(*chain_of_champions_100[-1], which='ds100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_grid_models:\n",
    "    train_batch(\n",
    "        which = 'ds20', \n",
    "        index_set=hp_idx,  \n",
    "        patience_limit=15, \n",
    "        epochs = 400\n",
    "        )\n",
    "\n",
    "    train_batch(\n",
    "        which = 'ds100', \n",
    "        index_set=hp_idx,  \n",
    "        patience_limit=10, \n",
    "        epochs = 400\n",
    "        )\n",
    "    \n",
    "    # find best performing model in each \n",
    "    best_model_grid_20 = None\n",
    "    best_model_grid_100 = None\n",
    "\n",
    "    best_r2_20 = -np.inf\n",
    "    best_r2_100 = -np.inf\n",
    "\n",
    "    for idx in hp_idx:\n",
    "        tm_20 = load_tag_model(idx, which='ds20')\n",
    "        _, r2_20 = tm_20.eval_model(data_ds20['valid_ds'], threshold=0.05)\n",
    "\n",
    "        tm_100 = load_tag_model(idx, which='ds100')\n",
    "        _, r2_100 = tm_100.eval_model(data_ds100['valid_ds'], threshold=0.05)\n",
    "\n",
    "        if r2_20 > best_r2_20: \n",
    "            best_r2_20 = r2_20\n",
    "            best_model_grid_20 = idx\n",
    "\n",
    "        if r2_100 > best_r2_100: \n",
    "            best_r2_100 = r2_100\n",
    "            best_model_grid_100 = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection.  \n",
    "\n",
    "if model_select == 'iterative':\n",
    "    tm_best_20 = tm_best_it_20\n",
    "    tm_best_100 = tm_best_it_100\n",
    "elif model_select == 'grid':\n",
    "    tm_best_20 = load_tag_model(best_model_grid_20, which='ds20')\n",
    "    tm_best_100 = load_tag_model(best_model_grid_100, which='ds100')\n",
    "elif isinstance(model_select, list): \n",
    "    tm_best_20 = load_tag_model(model_select[0], which='ds20')\n",
    "    tm_best_100 = load_tag_model(model_select[1], which='ds100')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "# we dissect the indices again \n",
    "fig, axs = plt.subplots(len(NL), len(NC), figsize=(15, 15))\n",
    "\n",
    "for i, nl in enumerate(NL): \n",
    "    for j, nc in enumerate(NC): \n",
    "        ax = axs[i,j]\n",
    "        for nh in NH: \n",
    "            ts = hist_dict[(nl, nc, nh)]['valid_r2']\n",
    "            L = len(ts)\n",
    "            epochs = np.arange(L)\n",
    "            eps = epochs[25:]\n",
    "            t = ts[25:]\n",
    "            l = str(nh) + \" hops\"\n",
    "            ax.plot(eps, t, label=l)\n",
    "            ax.legend()\n",
    "            ax.set_ylim(0.4,0.9)\n",
    "            ax.grid()\n",
    "            ax.set_title(f\"{nl} layers, {nc} channels\")\n",
    "\n",
    "plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'TAGModule' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtuple\u001b[39m(json\u001b[38;5;241m.\u001b[39mloads(\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m f\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m         )) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(training_folder)]\n\u001b[0;32m     15\u001b[0m idx_ds20 \u001b[38;5;241m=\u001b[39m all_indices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds20\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m df_size_perf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m---> 19\u001b[0m     [\n\u001b[0;32m     20\u001b[0m         (\n\u001b[0;32m     21\u001b[0m             idx, \n\u001b[0;32m     22\u001b[0m             \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m load_tag_model(idx, which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds20\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mparameters()), \n\u001b[0;32m     23\u001b[0m             read_hist_data(idx, which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds20\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_r2\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     24\u001b[0m         )\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m idx_ds20\n\u001b[0;32m     26\u001b[0m     ],\n\u001b[0;32m     27\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m display(df_size_perf)\n\u001b[0;32m     32\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(df_size_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m], df_size_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtuple\u001b[39m(json\u001b[38;5;241m.\u001b[39mloads(\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m f\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m         )) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(training_folder)]\n\u001b[0;32m     15\u001b[0m idx_ds20 \u001b[38;5;241m=\u001b[39m all_indices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds20\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m df_size_perf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m     19\u001b[0m     [\n\u001b[0;32m     20\u001b[0m         (\n\u001b[0;32m     21\u001b[0m             idx, \n\u001b[1;32m---> 22\u001b[0m             \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mload_tag_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mds20\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mparameters()), \n\u001b[0;32m     23\u001b[0m             read_hist_data(idx, which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds20\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_r2\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     24\u001b[0m         )\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m idx_ds20\n\u001b[0;32m     26\u001b[0m     ],\n\u001b[0;32m     27\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m display(df_size_perf)\n\u001b[0;32m     32\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(df_size_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m], df_size_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mload_tag_model\u001b[1;34m(idx, which)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tag_model\u001b[39m(idx, which: \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds20\u001b[39m\u001b[38;5;124m'\u001b[39m): \n\u001b[0;32m      6\u001b[0m     training_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhich\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtraining_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/model_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1015\u001b[0m                      map_location,\n\u001b[0;32m   1016\u001b[0m                      pickle_module,\n\u001b[0;32m   1017\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1018\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1427\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:1415\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'TAGModule' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "idx_ds20 = all_indices('ds20')\n",
    "df_size_perf = pd.DataFrame(\n",
    "    [\n",
    "        (\n",
    "            idx, \n",
    "            sum(p.numel() for p in load_tag_model(idx, which='ds20').parameters()), \n",
    "            read_hist_data(idx, which='ds20')['valid_r2'][-1]\n",
    "        )\n",
    "        for idx in idx_ds20\n",
    "    ],\n",
    "    columns=['idx', 'size', 'r2']\n",
    ")\n",
    "\n",
    "display(df_size_perf)\n",
    "\n",
    "plt.scatter(df_size_perf['size'], df_size_perf['r2'])\n",
    "plt.xlabel('Model size')\n",
    "plt.ylabel('R2 score')\n",
    "# add index labels\n",
    "# for i, txt in enumerate(df_size_perf['idx']):\n",
    "#     plt.annotate(txt, (df_size_perf['size'][i], df_size_perf['r2'][i]))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustration of in-/output \n",
    "\n",
    "def plot_graph(\n",
    "        data, node_labels, ax=None, \n",
    "        title=None, node_size=100, \n",
    "        cmap='RdYlGn',\n",
    "        vmin=None, vmax=None,\n",
    "        add_colorbar=False,\n",
    "        colorbar_label=None):\n",
    "    \n",
    "    edges = data.edge_index.numpy()\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(node_labels.shape[0]))\n",
    "    G.add_edges_from(edges.T)\n",
    "\n",
    "    pos = nx.kamada_kawai_layout(G)\n",
    "\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos, ax=ax,\n",
    "        node_color='black',  # ring color\n",
    "        node_size=node_size  + min(max(40., 0.4*node_size), 20.),  # slightly larger size for the ring\n",
    "    )\n",
    "    # Draw the graph\n",
    "    nodes = nx.draw_networkx_nodes(\n",
    "        G, pos, ax=ax, node_color=node_labels, \n",
    "        cmap=cmap, node_size=node_size,\n",
    "        vmin=vmin, vmax=vmax  # Add these parameters for consistent color scaling\n",
    "        )\n",
    "    edges = nx.draw_networkx_edges(G, pos, ax=ax)\n",
    "    \n",
    "    if add_colorbar:\n",
    "        # Add colorbar with percentage formatting\n",
    "        cbar = plt.colorbar(nodes, ax=ax)\n",
    "        if colorbar_label:\n",
    "            cbar.set_label(colorbar_label)\n",
    "        cbar.ax.yaxis.set_major_formatter(\n",
    "            PercentFormatter(xmax=1.0, decimals=0)\n",
    "            )\n",
    "\n",
    "def plt_snbs(data, pred_snbs, node_size=100):\n",
    "\n",
    "    P = data.x.numpy()\n",
    "    snbs = data.y.numpy()\n",
    "\n",
    "    # Create the plot with equal sizes and shared colorbar\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    gs = fig.add_gridspec(1, 3, width_ratios=[1, 1, 1.2])  # Slightly wider last subplot for colorbar\n",
    "\n",
    "    axs = [fig.add_subplot(gs[0, i]) for i in range(3)]\n",
    "\n",
    "    # Get the full range of values for consistent colormap\n",
    "    vmin = min(snbs.min(), pred_snbs.min())\n",
    "    vmax = max(snbs.max(), pred_snbs.max())\n",
    "\n",
    "\n",
    "    # Plot each graph\n",
    "\n",
    "    # first graph gets a purple/orange colormap \n",
    "    plot_graph(\n",
    "        data, P, ax=axs[0], \n",
    "        title='Grid, Source (purple) / Sink (red)', \n",
    "        vmin=-1, vmax=1, cmap=mcolors.ListedColormap(['purple', 'orange']), \n",
    "        node_size=node_size\n",
    "        )\n",
    "\n",
    "    plot_graph(data, snbs, ax=axs[1], title='True SNBS', vmin=vmin, vmax=vmax, \n",
    "        node_size=node_size)\n",
    "    nodes = plot_graph(data, pred_snbs, ax=axs[2], title='Predicted SNBS', \n",
    "                    vmin=vmin, vmax=vmax, add_colorbar=True, \n",
    "                    colorbar_label='SNBS', \n",
    "        node_size=node_size)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "data = data_ds100['train_ds'].dataset[0]\n",
    "pred_snbs = tag_module(data).detach().numpy()\n",
    "plt_snbs(data, pred_snbs, node_size=30)\n",
    "plt.savefig('sample_graph_100.png')\n",
    "plt.show()\n",
    "\n",
    "data = data_ds20['train_ds'].dataset[0]\n",
    "pred_snbs = tag_module(data).detach().numpy()\n",
    "plt_snbs(data, pred_snbs, node_size=100)\n",
    "plt.savefig('sample_graph_20.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uda_project_autumn_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
