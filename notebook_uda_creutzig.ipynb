{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user specific setup \n",
    "# this should work fine as default but you might want to store data \n",
    "# other places so feel free to change the paths here\n",
    "\n",
    "zip_folder = 'data_download_zip/' \n",
    "dl_data_folder = 'data/'\n",
    "\n",
    "base_url = 'https://zenodo.org/record/8204334/files/'\n",
    "\n",
    "ds20_url = base_url + 'ds20.zip'\n",
    "ds100_url = base_url + 'ds100.zip'\n",
    "\n",
    "ds20_zipfile = zip_folder + 'ds20.zip'\n",
    "ds100_zipfile = zip_folder  + 'ds100.zip'\n",
    "\n",
    "ds20_data_folder = dl_data_folder + 'ds20/'\n",
    "ds100_data_folder = dl_data_folder + 'ds100/'\n",
    "\n",
    "ds20_model_folder = 'models/ds20/'\n",
    "ds100_model_folder = 'models/ds100/'\n",
    "\n",
    "EDGE_WGT = 9. \n",
    "\n",
    "our_rando_seed = 417417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### \n",
    "#  Active user preferences \n",
    "# \n",
    "# Flags to activate/deactivate download, training, plotting etc \n",
    "#\n",
    "\n",
    "download_data = False # True always downloads/extracts data, False only when data is missing \n",
    "\n",
    "# we made an iterative process for training models\n",
    "# This can take quite some time, so you can instead also load pre-trained models.  \n",
    "make_iterative_models = True   \n",
    "\n",
    "# This runs simply through a grid of model hyperparameters and trains them all.\n",
    "# Also quite time intensive, and I think outdated now because the iterative process \n",
    "# works reasonably well.  \n",
    "make_grid_models = True  \n",
    "\n",
    "# Which model to select for evaluation/plotting \n",
    "# You can get the best \"iterative\" or \"grid\" or a tuple (nl, nc, nh) for a specific model \n",
    "\n",
    "model_select = \"iterative\" \n",
    "# model_select = [(7,40,7), (8,40,9)] \n",
    "\n",
    "plot_results = True   # Plots the results of the training.\n",
    "\n",
    "# Training setup.  \n",
    "NL = [4, 5, 6, 7] # number of layers to be trained \n",
    "NC = [20, 30, 40] # number of channels \n",
    "NH = [4, 5, 6, 7] # number of hops \n",
    "\n",
    "hp_idx = [\n",
    "    (nl, nc, nh) \n",
    "    for nl in NL \n",
    "    for nc in NC  \n",
    "    for nh in NH \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/envs/uda_project_autumn_2024/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict,  List, Callable\n",
    "import os \n",
    "import json \n",
    "from requests import get \n",
    "import zipfile\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import networkx as nx \n",
    "\n",
    "import torch \n",
    "from torch import tensor\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from torch_geometric.nn import TAGConv\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data \n",
    "# This will take maybe 30 seconds initially.  \n",
    "# Seecond time you should not notice the lag.  \n",
    "def mkdir_if_not_exist(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "def make_path_if_not_exist(file):\n",
    "    folder = os.path.dirname(file)\n",
    "    mkdir_if_not_exist(folder)\n",
    "\n",
    "def load_from_source(url, file, overwrite=False):\n",
    "    make_path_if_not_exist(file)\n",
    "    if not os.path.exists(file) or overwrite:\n",
    "        with open(file, 'wb') as f:\n",
    "            response = get(url)\n",
    "            f.write(response.content)\n",
    "            print(f\"Downloaded {url} to {file}\")\n",
    "            f.close()\n",
    "\n",
    "    else:\n",
    "        print(f\"File {file} already exists. Set overwrite=True to overwrite\")\n",
    "\n",
    "if download_data:\n",
    "    load_from_source(ds20_url, ds20_zipfile)\n",
    "    load_from_source(ds100_url, ds100_zipfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now extract from zip files and load the data \n",
    "if download_data: \n",
    "    def extract_zip(zipfile_name, folder):\n",
    "        zf = zipfile.ZipFile(zipfile_name)\n",
    "        zf.extractall(folder)\n",
    "        zf.close()\n",
    "\n",
    "    extract_zip(ds20_zipfile, dl_data_folder)\n",
    "    extract_zip(ds100_zipfile, dl_data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/envs/uda_project_autumn_2024/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# read in hd5 data \n",
    "def load_data(ds: str, type:str, data_folder:str, batch_size=500) -> DataLoader:  \n",
    "\n",
    "    folder_name = f\"{data_folder}/{ds}/{type}/\"\n",
    "\n",
    "    file_name_input = f\"{folder_name}/input_data.h5\"\n",
    "    file_name_snbs = f\"{folder_name}/snbs.h5\"\n",
    "\n",
    "    snbs_data = h5py.File(file_name_snbs, 'r')\n",
    "    input_data = h5py.File(file_name_input, 'r')['grids']\n",
    "\n",
    "    grid_data = [\n",
    "        Data(\n",
    "            x = tensor(np.array(id_v['node_features']).reshape(-1, 1), dtype=torch.float),\n",
    "            edge_index = tensor(np.array(id_v['edge_index']).T - 1, dtype=torch.long),\n",
    "            edge_attr = tensor(np.full(shape=id_v['edge_index'].shape[0], fill_value=EDGE_WGT), dtype=torch.float),\n",
    "            y = tensor(np.array(snbs_v), dtype=torch.float)\n",
    "        )\n",
    "        for id_v, snbs_v in zip(input_data.values(), snbs_data.values())\n",
    "    ]\n",
    "\n",
    "    return DataLoader(grid_data, batch_size=batch_size)\n",
    "\n",
    "def load_dataset(ds:str, data_folder: str) -> Dict[str, DataLoader]: \n",
    "    res= dict() \n",
    "    res['train_ds'] = load_data(ds=ds, type='train', data_folder=data_folder)\n",
    "    res['test_ds'] = load_data(ds=ds, type='test', data_folder=data_folder)\n",
    "    res['valid_ds'] = load_data(ds=ds, type='valid', data_folder=data_folder)\n",
    "    return res \n",
    "\n",
    "data_ds20 = load_dataset('ds20', dl_data_folder)\n",
    "data_ds100 = load_dataset('ds100', dl_data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[20, 1], edge_index=[2, 54], edge_attr=[54], y=[20])\n",
      "tensor([[-1.,  1.,  1., -1., -1.]])\n",
      "tensor([[0, 1, 0, 3, 0],\n",
      "        [1, 0, 3, 0, 5]])\n",
      "tensor([9., 9., 9., 9., 9.])\n",
      "tensor([1.0000, 1.0000, 0.8447, 1.0000, 0.9946])\n"
     ]
    }
   ],
   "source": [
    "# This is how the training data looks like\n",
    "\n",
    "print(data_ds20['train_ds'].dataset[0])\n",
    "print(data_ds100['train_ds'].dataset[0].x[:5].T) \n",
    "print(data_ds100['train_ds'].dataset[0].edge_index[:2,:5])\n",
    "print(data_ds100['train_ds'].dataset[0].edge_attr[:5])\n",
    "print(data_ds100['train_ds'].dataset[0].y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAGConvModule(torch.nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, K_hops, activation, batch_norm) -> None:\n",
    "        super().__init__()\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "        self.K_hops = K_hops\n",
    "        self.activation = activation\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.conv = TAGConv(\n",
    "            self.channels_in, \n",
    "            self.channels_out, \n",
    "            K=self.K_hops\n",
    "        )\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm_layer = torch.nn.BatchNorm1d(self.channels_out)\n",
    "\n",
    "    def forward(self, data, x):\n",
    "        # Add safety checks\n",
    "        num_nodes = x.size(0)\n",
    "        if data.edge_index.max() >= num_nodes:\n",
    "            raise ValueError(f\"Edge index contains invalid node indices. Max index: {data.edge_index.max()}, num nodes: {num_nodes}\")\n",
    "        \n",
    "        if data.edge_index.min() < 0:\n",
    "            raise ValueError(f\"Edge index contains negative indices: {data.edge_index.min()}\")\n",
    "            \n",
    "        # Apply convolution\n",
    "        x = self.conv(x, data.edge_index, data.edge_attr)\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm_layer(x)\n",
    "        \n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "class TAGNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        no_layers: int,\n",
    "        channels: List[int],\n",
    "        activation: List[Callable],\n",
    "        K_hops: List[int],\n",
    "        batch_norm: List[bool],\n",
    "        final_linear_layer: bool,\n",
    "        final_sigmoid_layer: bool = True, \n",
    "    ) -> None:\n",
    "        super(TAGNet, self).__init__()\n",
    "\n",
    "        self.no_layers = no_layers\n",
    "        self.channels = channels\n",
    "        self.activation = activation\n",
    "        self.K_hops = K_hops\n",
    "        self.batch_norm = batch_norm\n",
    "        self.final_linear_layer = final_linear_layer\n",
    "        self.final_sigmoid_layer = final_sigmoid_layer\n",
    "\n",
    "\n",
    "        self.convlist = torch.nn.ModuleList([\n",
    "            TAGConvModule(\n",
    "                channels_in=self.channels[i],\n",
    "                channels_out=self.channels[i+1],\n",
    "                activation=self.activation[i],\n",
    "                K_hops=self.K_hops[i],\n",
    "                batch_norm=self.batch_norm[i]\n",
    "            )\n",
    "            for i in range(self.no_layers)\n",
    "        ])\n",
    "\n",
    "        if self.final_linear_layer:\n",
    "            self.endLinear = torch.nn.Linear(self.channels[-1], 1)\n",
    "        if self.final_sigmoid_layer: \n",
    "            self.endSigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        \n",
    "        for conv in self.convlist:\n",
    "            x = conv(data, x)\n",
    "            \n",
    "        if self.final_linear_layer:\n",
    "            x = self.endLinear(x)\n",
    "        if self.final_sigmoid_layer: \n",
    "            x = self.endSigmoid(x)\n",
    "        \n",
    "        return x.squeeze(-1)  # Match target shape\n",
    "\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "class TAGModule(torch.nn.Module): \n",
    "    def __init__(\n",
    "        self, \n",
    "        channels, \n",
    "        activation,\n",
    "        K_hops,\n",
    "        batch_norm,\n",
    "        final_linear_layer, \n",
    "        final_sigmoid_layer\n",
    "    ) -> None:\n",
    "        \n",
    "        torch.manual_seed(our_rando_seed)\n",
    "        torch.cuda.manual_seed(our_rando_seed)\n",
    "        np.random.seed(our_rando_seed)        \n",
    "\n",
    "        super(TAGModule, self).__init__()\n",
    "        self.model = TAGNet(\n",
    "            no_layers=len(channels) - 1,\n",
    "            channels=channels,\n",
    "            activation=activation,\n",
    "            K_hops=K_hops,\n",
    "            batch_norm=batch_norm,\n",
    "            final_linear_layer=final_linear_layer,\n",
    "            final_sigmoid_layer=final_sigmoid_layer\n",
    "        )\n",
    "        self.device = torch.device('cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.criterion = MSELoss(reduction='mean')\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(), lr=3, momentum=0.9\n",
    "        ) \n",
    "\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=10, gamma=0.1\n",
    "        )\n",
    "\n",
    "        self.r2score = R2Score().to(self.device)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.model(x)\n",
    "\n",
    "    def r2score(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Calculate R² score from predictions and labels\n",
    "        Args:\n",
    "            predictions: torch.Tensor of predicted values\n",
    "            labels: torch.Tensor of true values\n",
    "        Returns:\n",
    "            R² score as a float\n",
    "        \"\"\"\n",
    "        # Mean of true values\n",
    "        y_mean = torch.mean(labels)\n",
    "        \n",
    "        # Total sum of squares\n",
    "        ss_tot = torch.sum((labels - y_mean) ** 2)\n",
    "        \n",
    "        # Residual sum of squares\n",
    "        ss_res = torch.sum((labels - predictions) ** 2)\n",
    "        \n",
    "        # R² calculation\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        return r2.item()\n",
    "\n",
    "    def train_epoch(self, data_loader, threshold=0.1):\n",
    "\n",
    "        self.model.train()\n",
    "        total_samples = 0\n",
    "        correct = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = torch.squeeze(self.model.forward(batch))\n",
    "            loss = self.criterion(outputs, batch.y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "\n",
    "            correct += torch.sum((torch.abs(outputs-batch.y) < threshold)).item()\n",
    "            total_samples += batch.y.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            y_true.append(batch.y.cpu().detach())\n",
    "            y_pred.append(outputs.cpu().detach())\n",
    "\n",
    "\n",
    "        accuracy = correct / total_samples\n",
    "\n",
    "        y_true = torch.cat(y_true)\n",
    "        y_pred = torch.cat(y_pred)\n",
    "        r2_score = self.r2score(y_pred, y_true)\n",
    "\n",
    "        return accuracy, r2_score\n",
    "\n",
    "    def eval_model(self, data_loader, threshold=0.1): \n",
    "        self.model.eval() \n",
    "        correct = 0 \n",
    "        all_labels = torch.Tensor(0).to(self.device)\n",
    "        all_preds = torch.Tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for _, (batch) in enumerate(data_loader):\n",
    "                batch.to(self.device)\n",
    "                outputs = torch.squeeze(self.model(batch))\n",
    "                correct += torch.sum(( torch.abs(outputs-batch.y) < threshold ))\n",
    "                all_labels = torch.cat([all_labels, batch.y])\n",
    "                all_preds = torch.cat([all_preds, outputs])\n",
    "\n",
    "        accuracy = correct / all_labels.shape[0]\n",
    "        r2_score = self.r2score(all_preds, all_labels)\n",
    "\n",
    "        return accuracy, r2_score\n",
    "    \n",
    "    def pred_v_actual(self, data_loader):\n",
    "        \"\"\"\n",
    "        Extract predictions and ground truth from a data loader\n",
    "        Args:\n",
    "            data_loader: PyTorch Geometric DataLoader\n",
    "        Returns:\n",
    "            tuple (predictions, ground_truth) as numpy arrays\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        all_labels = torch.Tensor(0).to(self.device)\n",
    "        all_preds = torch.Tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _, (batch) in enumerate(data_loader):\n",
    "                batch.to(self.device)\n",
    "                outputs = torch.squeeze(self.model(batch))\n",
    "                all_labels = torch.cat([all_labels, batch.y])\n",
    "                all_preds = torch.cat([all_preds, outputs])\n",
    "\n",
    "        # Convert to numpy arrays for easier handling\n",
    "        return all_preds.cpu().numpy(), all_labels.cpu().numpy()       \n",
    "\n",
    "# Update your model initialization\n",
    "tag_net = TAGNet(\n",
    "    no_layers=3,\n",
    "    channels=[1, 32, 32, 1],  # Make sure dimensions match\n",
    "    activation=[torch.nn.ReLU()] * 3,\n",
    "    K_hops=[2] * 3,\n",
    "    batch_norm=[True] * 3, \n",
    "    final_linear_layer=True\n",
    ")\n",
    "\n",
    "tag_module = TAGModule(\n",
    "    channels=[1, 30, 30, 1],\n",
    "    activation=[torch.nn.ReLU()] * 3,\n",
    "    K_hops=[3] * 3,\n",
    "    batch_norm=[True] * 3,\n",
    "    final_linear_layer=False,\n",
    "    final_sigmoid_layer=True\n",
    ")\n",
    "def triangle_channels(nl, nc):  \n",
    "    # we want to interpolate in a geometric manner between \n",
    "    # 1 and nc, over nl.  \n",
    "\n",
    "    log_int = np.linspace(0, 1, nl) \n",
    "    return [int(x) for x in np.ceil(nc**log_int)[1:]]\n",
    "\n",
    "\n",
    "def make_tag_module(num_layers, num_channels, num_hops): \n",
    "    return TAGModule(\n",
    "        channels=[1] + triangle_channels(num_layers, num_channels)  + [1],\n",
    "        activation=[torch.nn.ReLU()] * num_layers,\n",
    "        K_hops=[num_hops] * num_layers,\n",
    "        batch_norm=[True] * num_layers,\n",
    "        final_linear_layer=False,\n",
    "        final_sigmoid_layer=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(tag_module, data_loader, epochs, patience_limit=50, min_perc_increase=0, print_at=None):\n",
    "    best_so_far = -np.inf\n",
    "    patience_used=0\n",
    "    all_train_acc = [] \n",
    "    all_train_r2 = [] \n",
    "    all_test_acc = [] \n",
    "    all_test_r2 = [] \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_acc, train_r2 = tag_module.train_epoch(data_loader['train_ds'], threshold=0.05)\n",
    "        valid_acc, valid_r2 = tag_module.eval_model(data_loader['valid_ds'], threshold=0.05)\n",
    "        if valid_r2 > best_so_far * (1 + min_perc_increase/100):\n",
    "            best_so_far = valid_r2\n",
    "            patience_used = 0\n",
    "        else:\n",
    "            patience_used += 1\n",
    "            if patience_used > patience_limit :\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        if print_at: \n",
    "            if epoch % print_at == 0:\n",
    "                print(f\"Epoch {epoch}: Train accuracy: {train_acc:.2f}, Train R2: {train_r2:.2f}, Valid accuracy: {valid_acc:.2f}, Valid R2: {valid_r2:.2f}\")\n",
    "\n",
    "        all_train_acc.append(float(train_acc))\n",
    "        all_train_r2.append(float(train_r2))\n",
    "        all_test_acc.append(float(valid_acc))\n",
    "        all_test_r2.append(float(valid_r2))\n",
    "\n",
    "    hist = {\n",
    "        'train_acc': np.array(all_train_acc), \n",
    "        'train_r2': np.array(all_train_r2), \n",
    "        'valid_acc': np.array(all_test_acc), \n",
    "        'valid_r2': np.array(all_test_r2), \n",
    "    }\n",
    "    return hist \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "        index_set, \n",
    "        which='ds20', \n",
    "        overwrite=False, \n",
    "        epochs=400, \n",
    "        patience_limit=400, \n",
    "        min_perc_increase = 0 \n",
    "        ): \n",
    "    hist_dict = dict()\n",
    "    model_dict = dict()\n",
    "    \n",
    "    training_folder = f'trained_models_{which}/'\n",
    "    hist_folder = f'training_hist_{which}/'\n",
    "\n",
    "    # make folders if not exist \n",
    "    mkdir_if_not_exist(training_folder)\n",
    "    mkdir_if_not_exist(hist_folder)\n",
    "\n",
    "    for idx in index_set: \n",
    "        training_file_name = training_folder + f'model_{str(idx)}.pth'\n",
    "        if os.path.exists(training_file_name) and not overwrite:\n",
    "            print(f\"Model {str(idx)} already trained. Skipping\")\n",
    "            continue\n",
    "\n",
    "        # find data set \n",
    "        if which == 'ds20': \n",
    "            data = data_ds20 \n",
    "        elif which == 'ds100': \n",
    "            data = data_ds100\n",
    "        else:\n",
    "            raise ValueError('Unknown parameter which ' + which)\n",
    "\n",
    "        print(f'Training {str(idx)}..')\n",
    "        model_dict[idx] = make_tag_module(\n",
    "            num_layers=idx[0], \n",
    "            num_channels=idx[1], \n",
    "            num_hops = idx[2]\n",
    "            )\n",
    "        hist_dict[idx] = train_model(\n",
    "            model_dict[idx], \n",
    "            data, \n",
    "            epochs = epochs, \n",
    "            patience_limit=patience_limit, \n",
    "            print_at=50, \n",
    "            min_perc_increase = min_perc_increase\n",
    "        )\n",
    "\n",
    "        torch.save(model_dict[idx], training_folder + f'model_{str(idx)}.pth') \n",
    "        # transform hist dict into json and save too \n",
    "        tmp_dict = {\n",
    "            k: list(v) \n",
    "            for k, v in hist_dict[idx].items() \n",
    "        }\n",
    "        import json \n",
    "        json.dump(\n",
    "            tmp_dict, \n",
    "            open(f'{hist_folder}/{str(idx)}.pth', 'w') \n",
    "            )\n",
    "\n",
    "    print(\"done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_hp_tuning(\n",
    "\n",
    "    nl_init = 2, \n",
    "    nc_init = 10, \n",
    "    nh_init = 1, \n",
    "\n",
    "    inc_nl = 1, \n",
    "    inc_nc = 10,\n",
    "    inc_nh = 1,\n",
    "\n",
    "    max_depth = 20, \n",
    "    patience = 15,\n",
    "    min_perc_increase=5, \n",
    "    overwrite = False, \n",
    "    which = 'ds20'\n",
    "): \n",
    "    nl, nc, nh = (nl_init, nc_init, nh_init)\n",
    "    chain_of_champions = [(nl, nc, nh)]\n",
    "\n",
    "    hit_limit = False \n",
    "\n",
    "    import json \n",
    "    def get_r2(idx, which='ds20'): \n",
    "        hist_folder = f'training_hist_{which}'\n",
    "        perf = json.load(open(f'{hist_folder}/{str(idx)}.pth'))\n",
    "        return perf['valid_r2'][-1]\n",
    "\n",
    "    def train_and_get_r2(idx, which='ds20'): \n",
    "        train_batch([idx], which=which, epochs=400, patience_limit=patience, min_perc_increase = min_perc_increase, overwrite=overwrite) \n",
    "        return(get_r2(idx, which))\n",
    "\n",
    "    def argmax_dict(my_dict): \n",
    "        \"\"\"returns key with largest value.  Only works with comparable values..\"\"\"\n",
    "        list_idx_max = np.argmax(list(my_dict.values()))\n",
    "        return list(my_dict.keys())[list_idx_max]\n",
    "\n",
    "    counter= 0 \n",
    "    max_count = max_depth \n",
    "\n",
    "    while not hit_limit and counter < max_count: \n",
    "        curr_perf = {\n",
    "            idx: train_and_get_r2(idx, which) \n",
    "            for idx in [\n",
    "                (nl, nc, nh), \n",
    "                (nl + inc_nl, nc, nh), \n",
    "                (nl, nc + inc_nc, nh), \n",
    "                (nl, nc, nh + inc_nh), \n",
    "            ]\n",
    "        }\n",
    "        idx_with_max_perf = argmax_dict(curr_perf) \n",
    "        if idx_with_max_perf == (nl, nc, nh): \n",
    "            print(f\"No further improvement registered.  Final performance {curr_perf[idx_with_max_perf]}.\")\n",
    "            hit_limit=True \n",
    "        else: \n",
    "            nl, nc, nh = idx_with_max_perf \n",
    "            chain_of_champions.append(idx_with_max_perf)\n",
    "            print(f\"=====  New champion: {idx_with_max_perf} =====\")\n",
    "            counter += 1\n",
    "\n",
    "    return chain_of_champions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hist_data(idx, which: str='ds20'):\n",
    "    hist_folder = f'training_hist_{which}'\n",
    "    return json.load(open(f'{hist_folder}/{str(idx)}.pth'))\n",
    "\n",
    "def load_tag_model(idx, which: str='ds20'): \n",
    "    training_folder = f'trained_models_{which}'\n",
    "    return torch.load(f'{training_folder}/model_{str(idx)}.pth')\n",
    "\n",
    "def all_indices(which: str='ds20'): \n",
    "    training_folder = f'trained_models_{which}'\n",
    "    return [tuple(json.loads(\n",
    "        '[' + f.replace('model_','').split('.')[0][1:-1] + ']'\n",
    "        )) for f in os.listdir(training_folder)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (3, 10, 2)..\n",
      "Epoch 0: Train accuracy: 0.17, Train R2: -2.75, Valid accuracy: 0.23, Valid R2: -1.46\n",
      "Epoch 50: Train accuracy: 0.56, Train R2: 0.46, Valid accuracy: 0.58, Valid R2: 0.46\n",
      "Epoch 100: Train accuracy: 0.56, Train R2: 0.49, Valid accuracy: 0.56, Valid R2: 0.50\n",
      "Epoch 150: Train accuracy: 0.57, Train R2: 0.51, Valid accuracy: 0.56, Valid R2: 0.52\n",
      "Early stopping at epoch 162\n",
      "done.\n",
      "Training (4, 10, 2)..\n",
      "Epoch 0: Train accuracy: 0.21, Train R2: -1.98, Valid accuracy: 0.22, Valid R2: -1.74\n",
      "Epoch 50: Train accuracy: 0.56, Train R2: 0.51, Valid accuracy: 0.56, Valid R2: 0.52\n",
      "Epoch 100: Train accuracy: 0.57, Train R2: 0.54, Valid accuracy: 0.58, Valid R2: 0.55\n",
      "Epoch 150: Train accuracy: 0.59, Train R2: 0.56, Valid accuracy: 0.59, Valid R2: 0.56\n",
      "Early stopping at epoch 153\n",
      "done.\n",
      "Training (3, 20, 2)..\n",
      "Epoch 0: Train accuracy: 0.23, Train R2: -2.07, Valid accuracy: 0.20, Valid R2: -1.94\n",
      "Epoch 50: Train accuracy: 0.57, Train R2: 0.46, Valid accuracy: 0.56, Valid R2: 0.46\n",
      "Epoch 100: Train accuracy: 0.56, Train R2: 0.48, Valid accuracy: 0.55, Valid R2: 0.49\n",
      "Early stopping at epoch 108\n",
      "done.\n",
      "Training (3, 10, 3)..\n",
      "Epoch 0: Train accuracy: 0.19, Train R2: -2.66, Valid accuracy: 0.20, Valid R2: -1.25\n",
      "Epoch 50: Train accuracy: 0.57, Train R2: 0.49, Valid accuracy: 0.52, Valid R2: 0.47\n",
      "Early stopping at epoch 63\n",
      "done.\n",
      "=====  New champion: (4, 10, 2) =====\n",
      "Training (4, 10, 2)..\n",
      "Epoch 0: Train accuracy: 0.21, Train R2: -1.98, Valid accuracy: 0.22, Valid R2: -1.74\n",
      "Epoch 50: Train accuracy: 0.56, Train R2: 0.51, Valid accuracy: 0.56, Valid R2: 0.52\n",
      "Epoch 100: Train accuracy: 0.57, Train R2: 0.54, Valid accuracy: 0.58, Valid R2: 0.55\n",
      "Epoch 150: Train accuracy: 0.59, Train R2: 0.56, Valid accuracy: 0.59, Valid R2: 0.56\n",
      "Early stopping at epoch 153\n",
      "done.\n",
      "Training (5, 10, 2)..\n",
      "Epoch 0: Train accuracy: 0.21, Train R2: -2.03, Valid accuracy: 0.22, Valid R2: -1.03\n",
      "Epoch 50: Train accuracy: 0.57, Train R2: 0.50, Valid accuracy: 0.57, Valid R2: 0.49\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Iterative hyper parameter exploration \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_iterative_models: \n\u001b[0;32m----> 4\u001b[0m     chain_of_champions_20 \u001b[38;5;241m=\u001b[39m \u001b[43miterative_hp_tuning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mds20\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnl_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnc_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnh_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_perc_increase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     chain_of_champions_100 \u001b[38;5;241m=\u001b[39m iterative_hp_tuning(\n\u001b[1;32m     11\u001b[0m         which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds100\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     12\u001b[0m         nl_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, nc_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, nh_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[1;32m     13\u001b[0m         min_perc_increase\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m, \n\u001b[1;32m     14\u001b[0m         overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \n\u001b[1;32m     15\u001b[0m         )\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# save the champions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 41\u001b[0m, in \u001b[0;36miterative_hp_tuning\u001b[0;34m(nl_init, nc_init, nh_init, inc_nl, inc_nc, inc_nh, max_depth, patience, min_perc_increase, overwrite, which)\u001b[0m\n\u001b[1;32m     38\u001b[0m max_count \u001b[38;5;241m=\u001b[39m max_depth \n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hit_limit \u001b[38;5;129;01mand\u001b[39;00m counter \u001b[38;5;241m<\u001b[39m max_count: \n\u001b[0;32m---> 41\u001b[0m     curr_perf \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m         idx: train_and_get_r2(idx, which) \n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m     44\u001b[0m             (nl, nc, nh), \n\u001b[1;32m     45\u001b[0m             (nl \u001b[38;5;241m+\u001b[39m inc_nl, nc, nh), \n\u001b[1;32m     46\u001b[0m             (nl, nc \u001b[38;5;241m+\u001b[39m inc_nc, nh), \n\u001b[1;32m     47\u001b[0m             (nl, nc, nh \u001b[38;5;241m+\u001b[39m inc_nh), \n\u001b[1;32m     48\u001b[0m         ]\n\u001b[1;32m     49\u001b[0m     }\n\u001b[1;32m     50\u001b[0m     idx_with_max_perf \u001b[38;5;241m=\u001b[39m argmax_dict(curr_perf) \n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx_with_max_perf \u001b[38;5;241m==\u001b[39m (nl, nc, nh): \n",
      "Cell \u001b[0;32mIn[11], line 42\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m max_count \u001b[38;5;241m=\u001b[39m max_depth \n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hit_limit \u001b[38;5;129;01mand\u001b[39;00m counter \u001b[38;5;241m<\u001b[39m max_count: \n\u001b[1;32m     41\u001b[0m     curr_perf \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 42\u001b[0m         idx: \u001b[43mtrain_and_get_r2\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhich\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m     44\u001b[0m             (nl, nc, nh), \n\u001b[1;32m     45\u001b[0m             (nl \u001b[38;5;241m+\u001b[39m inc_nl, nc, nh), \n\u001b[1;32m     46\u001b[0m             (nl, nc \u001b[38;5;241m+\u001b[39m inc_nc, nh), \n\u001b[1;32m     47\u001b[0m             (nl, nc, nh \u001b[38;5;241m+\u001b[39m inc_nh), \n\u001b[1;32m     48\u001b[0m         ]\n\u001b[1;32m     49\u001b[0m     }\n\u001b[1;32m     50\u001b[0m     idx_with_max_perf \u001b[38;5;241m=\u001b[39m argmax_dict(curr_perf) \n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx_with_max_perf \u001b[38;5;241m==\u001b[39m (nl, nc, nh): \n",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m, in \u001b[0;36miterative_hp_tuning.<locals>.train_and_get_r2\u001b[0;34m(idx, which)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_get_r2\u001b[39m(idx, which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds20\u001b[39m\u001b[38;5;124m'\u001b[39m): \n\u001b[0;32m---> 29\u001b[0m     \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhich\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_perc_increase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_perc_increase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(get_r2(idx, which))\n",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(index_set, which, overwrite, epochs, patience_limit, min_perc_increase)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(idx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m model_dict[idx] \u001b[38;5;241m=\u001b[39m make_tag_module(\n\u001b[1;32m     35\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39midx[\u001b[38;5;241m0\u001b[39m], \n\u001b[1;32m     36\u001b[0m     num_channels\u001b[38;5;241m=\u001b[39midx[\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m     37\u001b[0m     num_hops \u001b[38;5;241m=\u001b[39m idx[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     38\u001b[0m     )\n\u001b[0;32m---> 39\u001b[0m hist_dict[idx] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_perc_increase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_perc_increase\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model_dict[idx], training_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(idx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# transform hist dict into json and save too \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(tag_module, data_loader, epochs, patience_limit, min_perc_increase, print_at)\u001b[0m\n\u001b[1;32m      7\u001b[0m all_test_r2 \u001b[38;5;241m=\u001b[39m [] \n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 10\u001b[0m     train_acc, train_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mtag_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_ds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     valid_acc, valid_r2 \u001b[38;5;241m=\u001b[39m tag_module\u001b[38;5;241m.\u001b[39meval_model(data_loader[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_ds\u001b[39m\u001b[38;5;124m'\u001b[39m], threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m valid_r2 \u001b[38;5;241m>\u001b[39m best_so_far \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m min_perc_increase\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m):\n",
      "Cell \u001b[0;32mIn[8], line 163\u001b[0m, in \u001b[0;36mTAGModule.train_epoch\u001b[0;34m(self, data_loader, threshold)\u001b[0m\n\u001b[1;32m    160\u001b[0m y_true \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    161\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m    164\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/uda_project_autumn_2024/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/uda_project_autumn_2024/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/uda_project_autumn_2024/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/uda_project_autumn_2024/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[0;32m~/miniconda3/envs/uda_project_autumn_2024/lib/python3.10/site-packages/torch_geometric/data/batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/uda_project_autumn_2024/lib/python3.10/site-packages/torch_geometric/data/collate.py:109\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m value, slices, incs \u001b[38;5;241m=\u001b[39m \u001b[43m_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# If parts of the data are already on GPU, make sure that auxiliary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# data like `batch` or `ptr` are also created on GPU:\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_cuda:\n",
      "File \u001b[0;32m~/miniconda3/envs/uda_project_autumn_2024/lib/python3.10/site-packages/torch_geometric/data/collate.py:166\u001b[0m, in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cat_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    165\u001b[0m     values \u001b[38;5;241m=\u001b[39m [value\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[0;32m--> 166\u001b[0m sizes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([value\u001b[38;5;241m.\u001b[39msize(cat_dim \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values])\n\u001b[1;32m    167\u001b[0m slices \u001b[38;5;241m=\u001b[39m cumsum(sizes)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m increment:\n",
      "File \u001b[0;32m~/miniconda3/envs/uda_project_autumn_2024/lib/python3.10/site-packages/torch_geometric/data/collate.py:166\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cat_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    165\u001b[0m     values \u001b[38;5;241m=\u001b[39m [value\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[0;32m--> 166\u001b[0m sizes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values])\n\u001b[1;32m    167\u001b[0m slices \u001b[38;5;241m=\u001b[39m cumsum(sizes)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m increment:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterative hyper parameter exploration \n",
    "\n",
    "if make_iterative_models: \n",
    "    chain_of_champions_20 = iterative_hp_tuning(\n",
    "        which='ds20', \n",
    "        nl_init=3, nc_init = 10, nh_init=2, \n",
    "        min_perc_increase=0.5, patience = 15, \n",
    "        overwrite=True \n",
    "        )\n",
    "    chain_of_champions_100 = iterative_hp_tuning(\n",
    "        which='ds100', \n",
    "        nl_init=3, nc_init = 10, nh_init=2, \n",
    "        min_perc_increase=0.5, patience = 15, \n",
    "        overwrite=True \n",
    "        )\n",
    "    # save the champions\n",
    "    json.dump(chain_of_champions_20, open('champions_20.json', 'w'))\n",
    "    json.dump(chain_of_champions_100, open('champions_100.json', 'w'))\n",
    "else:\n",
    "    chain_of_champions_20 = json.load(open('champions_20.json'))\n",
    "    chain_of_champions_100 = json.load(open('champions_100.json'))\n",
    "\n",
    "tm_best_it_20 = load_tag_model(chain_of_champions_20[-1], which='ds20')\n",
    "tm_best_it_100 = load_tag_model(chain_of_champions_100[-1], which='ds100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_grid_models:\n",
    "    train_batch(\n",
    "        which = 'ds20', \n",
    "        index_set=hp_idx,  \n",
    "        min_perc_increase=0.1, \n",
    "        patience_limit=5, \n",
    "        epochs = 400, \n",
    "        overwrite=True \n",
    "        )\n",
    "\n",
    "    train_batch(\n",
    "        which = 'ds100', \n",
    "        index_set=hp_idx,  \n",
    "        min_perc_increase=0.1, \n",
    "        patience_limit=5, \n",
    "        epochs = 400, \n",
    "        overwrite=True\n",
    "        )\n",
    "    \n",
    "    # find best performing model in each \n",
    "    best_model_grid_20 = None\n",
    "    best_model_grid_100 = None\n",
    "\n",
    "    best_r2_20 = -np.inf\n",
    "    best_r2_100 = -np.inf\n",
    "\n",
    "    for idx in hp_idx:\n",
    "        tm_20 = load_tag_model(idx, which='ds20')\n",
    "        _, r2_20 = tm_20.eval_model(data_ds20['valid_ds'], threshold=0.05)\n",
    "\n",
    "        tm_100 = load_tag_model(idx, which='ds100')\n",
    "        _, r2_100 = tm_100.eval_model(data_ds100['valid_ds'], threshold=0.05)\n",
    "\n",
    "        if r2_20 > best_r2_20: \n",
    "            best_r2_20 = r2_20\n",
    "            best_model_grid_20 = idx\n",
    "\n",
    "        if r2_100 > best_r2_100: \n",
    "            best_r2_100 = r2_100\n",
    "            best_model_grid_100 = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection.  \n",
    "\n",
    "if model_select == 'iterative':\n",
    "    tm_best_20 = tm_best_it_20\n",
    "    tm_best_100 = tm_best_it_100\n",
    "elif model_select == 'grid':\n",
    "    tm_best_20 = load_tag_model(best_model_grid_20, which='ds20')\n",
    "    tm_best_100 = load_tag_model(best_model_grid_100, which='ds100')\n",
    "elif isinstance(model_select, list): \n",
    "    tm_best_20 = load_tag_model(model_select[0], which='ds20')\n",
    "    tm_best_100 = load_tag_model(model_select[1], which='ds100')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt \n",
    "# # we dissect the indices again \n",
    "# fig, axs = plt.subplots(len(NL), len(NC), figsize=(15, 15))\n",
    "\n",
    "# for i, nl in enumerate(NL): \n",
    "#     for j, nc in enumerate(NC): \n",
    "#         ax = axs[i,j]\n",
    "#         for nh in NH: \n",
    "#             ts = hist_dict[(nl, nc, nh)]['valid_r2']\n",
    "#             L = len(ts)\n",
    "#             epochs = np.arange(L)\n",
    "#             eps = epochs[25:]\n",
    "#             t = ts[25:]\n",
    "#             l = str(nh) + \" hops\"\n",
    "#             ax.plot(eps, t, label=l)\n",
    "#             ax.legend()\n",
    "#             ax.set_ylim(0.4,0.9)\n",
    "#             ax.grid()\n",
    "#             ax.set_title(f\"{nl} layers, {nc} channels\")\n",
    "\n",
    "# plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_ds20 = all_indices('ds20')\n",
    "# df_size_perf = pd.DataFrame(\n",
    "#     [\n",
    "#         (\n",
    "#             idx, \n",
    "#             sum(p.numel() for p in load_tag_model(idx, which='ds20').parameters()), \n",
    "#             read_hist_data(idx, which='ds20')['valid_r2'][-1]\n",
    "#         )\n",
    "#         for idx in idx_ds20\n",
    "#     ],\n",
    "#     columns=['idx', 'size', 'r2']\n",
    "# )\n",
    "\n",
    "# display(df_size_perf)\n",
    "\n",
    "# plt.scatter(df_size_perf['size'], df_size_perf['r2'])\n",
    "# plt.xlabel('Model size')\n",
    "# plt.ylabel('R2 score')\n",
    "# # add index labels\n",
    "# # for i, txt in enumerate(df_size_perf['idx']):\n",
    "# #     plt.annotate(txt, (df_size_perf['size'][i], df_size_perf['r2'][i]))\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # illustration of in-/output \n",
    "\n",
    "# def plot_graph(\n",
    "#         data, node_labels, ax=None, \n",
    "#         title=None, node_size=100, \n",
    "#         cmap='RdYlGn',\n",
    "#         vmin=None, vmax=None,\n",
    "#         add_colorbar=False,\n",
    "#         colorbar_label=None):\n",
    "    \n",
    "#     edges = data.edge_index.numpy()\n",
    "#     if ax is None:\n",
    "#         fig, ax = plt.subplots()\n",
    "    \n",
    "#     ax.set_title(title)\n",
    "#     ax.set_aspect('equal')\n",
    "\n",
    "#     G = nx.Graph()\n",
    "#     G.add_nodes_from(range(node_labels.shape[0]))\n",
    "#     G.add_edges_from(edges.T)\n",
    "\n",
    "#     pos = nx.kamada_kawai_layout(G)\n",
    "\n",
    "#     nx.draw_networkx_nodes(\n",
    "#         G, pos, ax=ax,\n",
    "#         node_color='black',  # ring color\n",
    "#         node_size=node_size  + min(max(40., 0.4*node_size), 20.),  # slightly larger size for the ring\n",
    "#     )\n",
    "#     # Draw the graph\n",
    "#     nodes = nx.draw_networkx_nodes(\n",
    "#         G, pos, ax=ax, node_color=node_labels, \n",
    "#         cmap=cmap, node_size=node_size,\n",
    "#         vmin=vmin, vmax=vmax  # Add these parameters for consistent color scaling\n",
    "#         )\n",
    "#     edges = nx.draw_networkx_edges(G, pos, ax=ax)\n",
    "    \n",
    "#     if add_colorbar:\n",
    "#         # Add colorbar with percentage formatting\n",
    "#         cbar = plt.colorbar(nodes, ax=ax)\n",
    "#         if colorbar_label:\n",
    "#             cbar.set_label(colorbar_label)\n",
    "#         cbar.ax.yaxis.set_major_formatter(\n",
    "#             PercentFormatter(xmax=1.0, decimals=0)\n",
    "#             )\n",
    "\n",
    "# def plt_snbs(data, pred_snbs, node_size=100):\n",
    "\n",
    "#     P = data.x.numpy()\n",
    "#     snbs = data.y.numpy()\n",
    "\n",
    "#     # Create the plot with equal sizes and shared colorbar\n",
    "#     fig = plt.figure(figsize=(15, 5))\n",
    "#     gs = fig.add_gridspec(1, 3, width_ratios=[1, 1, 1.2])  # Slightly wider last subplot for colorbar\n",
    "\n",
    "#     axs = [fig.add_subplot(gs[0, i]) for i in range(3)]\n",
    "\n",
    "#     # Get the full range of values for consistent colormap\n",
    "#     vmin = min(snbs.min(), pred_snbs.min())\n",
    "#     vmax = max(snbs.max(), pred_snbs.max())\n",
    "\n",
    "\n",
    "#     # Plot each graph\n",
    "\n",
    "#     # first graph gets a purple/orange colormap \n",
    "#     plot_graph(\n",
    "#         data, P, ax=axs[0], \n",
    "#         title='Grid, Source (purple) / Sink (red)', \n",
    "#         vmin=-1, vmax=1, cmap=mcolors.ListedColormap(['purple', 'orange']), \n",
    "#         node_size=node_size\n",
    "#         )\n",
    "\n",
    "#     plot_graph(data, snbs, ax=axs[1], title='True SNBS', vmin=vmin, vmax=vmax, \n",
    "#         node_size=node_size)\n",
    "#     nodes = plot_graph(data, pred_snbs, ax=axs[2], title='Predicted SNBS', \n",
    "#                     vmin=vmin, vmax=vmax, add_colorbar=True, \n",
    "#                     colorbar_label='SNBS', \n",
    "#         node_size=node_size)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "\n",
    "# data = data_ds100['train_ds'].dataset[0]\n",
    "# pred_snbs = tag_module(data).detach().numpy()\n",
    "# plt_snbs(data, pred_snbs, node_size=30)\n",
    "# plt.savefig('sample_graph_100.png')\n",
    "# plt.show()\n",
    "\n",
    "# data = data_ds20['train_ds'].dataset[0]\n",
    "# pred_snbs = tag_module(data).detach().numpy()\n",
    "# plt_snbs(data, pred_snbs, node_size=100)\n",
    "# plt.savefig('sample_graph_20.png')\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uda_project_autumn_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
